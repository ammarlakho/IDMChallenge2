{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "import unicodedata\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Add the train and test dataset files in the same folder as this .ipynb file for this to work or provide full path to these 2 files.\n",
    "train_filename = \"Trainset.csv\"\n",
    "test_filename = \"Testset without answer.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing Methods</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_texts(texts):\n",
    "    s = []\n",
    "    textList = texts.tolist()\n",
    "    for text in textList:\n",
    "        s_list = [preprocess_one_text(text)]\n",
    "        str_ = ' '.join(s_list)   \n",
    "        s.append(str_)     \n",
    "    return s\n",
    "\n",
    "def preprocess_one_text(text):\n",
    "    text = remove_accented_chars(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extra_whitespace_tabs(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = get_stem(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "    \n",
    "def remove_extra_whitespace_tabs(text):\n",
    "    pattern = r'^\\s*|\\s\\s*'\n",
    "    return re.sub(pattern, ' ', text).strip()    \n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return new_text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pat = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pat, '', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def get_stem(text):\n",
    "    stemmer = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    t = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    text = ' '.join(t)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helper Methods to make code cleaner</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the texts and labels from the training data as a tuple of 2 arrays\n",
    "def get_train_data(filename):\n",
    "    with open(filename,'r',encoding=\"ISO-8859-1\") as file:\n",
    "        csvDict = csv.DictReader(file)\n",
    "        labels, texts = [], []\n",
    "        for row in csvDict:\n",
    "            labels.append(row[\"rating\"])\n",
    "            texts.append(row[\"review\"])\n",
    "        labels = np.array(labels)\n",
    "        texts = np.array(texts)\n",
    "        return (texts, labels)\n",
    "    \n",
    "# Get the texts and ids from the test data as a tuple of 2 arrays\n",
    "def get_test_data(filename):\n",
    "     with open(filename,'r',encoding=\"ISO-8859-1\") as file:\n",
    "        csvDict = csv.DictReader(file)\n",
    "        texts, ids = [], []\n",
    "        for row in csvDict:\n",
    "            ids.append(row[\"id\"])\n",
    "            texts.append(row[\"review\"])\n",
    "        ids = np.array(ids)\n",
    "        texts = np.array(texts)\n",
    "        return (texts, ids)\n",
    "\n",
    "# Write the given ids array and predictions array to the given filename\n",
    "def write_csv(ids, pred, filename):\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"id\", \"rating\"])\n",
    "        rows = np.vstack((ids, pred)).T\n",
    "        writer.writerows(rows)\n",
    "        print(\"Wrote to a CSV file!\")\n",
    "\n",
    "# Encode the labels(good, average, poor) to (2, 1, 0)\n",
    "def encode(labels):\n",
    "    labels = np.where(labels==\"poor\", 0, labels) \n",
    "    labels = np.where(labels==\"average\", 1, labels) \n",
    "    labels = np.where(labels==\"good\", 2, labels) \n",
    "    labels = labels.astype(np.int64)\n",
    "    return labels\n",
    "\n",
    "# Decode the labels(0, 1, 2) to (good, average, poor)\n",
    "def decode(labels):\n",
    "    labels = labels.astype(str)\n",
    "    labels = np.where(labels==\"0\", \"poor\", labels) \n",
    "    labels = np.where(labels==\"1\", \"average\", labels) \n",
    "    labels = np.where(labels==\"2\", \"good\", labels) \n",
    "    return labels\n",
    "\n",
    "\n",
    "# Get the feature vector depending on various parameters\n",
    "def get_features(texts, vocab, vec_type, features, min_gram, max_gram):\n",
    "    if vec_type=='c':\n",
    "        print(\"using COUNT\")\n",
    "        vectorizer = CountVectorizer(ngram_range=(min_gram , max_gram), max_features=features) \n",
    "    if vec_type =='t':\n",
    "        print(\"using TFFF\")\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(min_gram , max_gram), max_features=features)\n",
    "    vectorizer.fit(vocab)\n",
    "    features = vectorizer.transform(texts)\n",
    "    features_nd = features.toarray()\n",
    "    return features_nd\n",
    "\n",
    "# Get the vectorizer(Useful for analyzing the type of words in the features)\n",
    "def get_vectorizer(texts, vocab, vec_type, features, min_gram, max_gram):\n",
    "    if type=='c':\n",
    "        print(\"using COUNT\")\n",
    "        vectorizer = CountVectorizer(ngram_range=(min_gram , max_gram), max_features=features) \n",
    "    if type =='t':\n",
    "        print(\"using TFFF\")\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(min_gram , max_gram), max_features=features)\n",
    "    vectorizer.fit(vocab)\n",
    "    return vectorizer\n",
    "    \n",
    "\n",
    "    \n",
    "# Sample CLassifiers for RF and Gradient Boosting\n",
    "# clf = RandomForestClassifier(n_estimators=200, criterion=\"gini\", max_depth=1000, random_state=0)\n",
    "\n",
    "# clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100,  criterion='friedman_mse', max_depth=3, random_state=0)\n",
    "\n",
    "# clf = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1, criterion='mse', max_depth=3, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Method for trying out various configurations of the vectorizer</h3>\n",
    "<p>Writing the accuracy value achieved(on cross validation on the training set) alongside the no. of features, the ngram range, the type of vectorizer to a csv file made it easy to search for the models with the best settings. These settings were used to train models that were used to predict labels for the test set. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_everything():\n",
    "    texts, labels = get_train_data(train_filename)\n",
    "    vectorizers = ['c', 't']\n",
    "    min_g = [0, 1]\n",
    "    clf = ComplementNB() #Can use any algorithm here\n",
    "    with open(\"CompNBModels.csv\", 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"vectorizer\", \"features\", \"ngrams\", \"accuracy\"])\n",
    "        for vec in vectorizers: #Vectorizer is either Count or TF-IDF\n",
    "            for f in range(2000, 4001, 2000): #Features ranging from 2000 to 80,000\n",
    "                for m in min_g:\n",
    "                    for ng in range(1,11): #ngram ranging from (0/1 to 10)\n",
    "                        train_X, train_Y = get_features(texts, texts, vec, f, m, ng), encode(labels)\n",
    "                        scores = cross_val_score(clf, train_X, train_Y, cv=4)\n",
    "                        acc = scores.mean()\n",
    "                        writer.writerow([vec, f, str(m)+'|'+str(ng), acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preparation and Choosing Vectorizer Configurations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using COUNT\n",
      "using COUNT\n"
     ]
    }
   ],
   "source": [
    "vec_type = 'c'\n",
    "features = 31000\n",
    "min_n, max_n = 0, 4\n",
    "texts, labels = get_train_data(train_filename)\n",
    "# Not doing any preproccessing since it negatively impacts accuracy\n",
    "# texts = preprocess_all_texts(texts) \n",
    "train_X, train_Y = get_features(texts, texts, vec_type, features, min_n, max_n), encode(labels)\n",
    "raw_test_X = get_test_data(test_filename)[0]\n",
    "# Not doing any preproccessing since it negatively impacts accuracy\n",
    "# raw_test_X = preprocess_all_texts(raw_test_X)\n",
    "test_X = get_features(raw_test_X, texts, vec_type, features, min_n, max_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross validation on training set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75844156 0.75324675 0.78571429 0.74382315]\n",
      "0.7603064360866703\n"
     ]
    }
   ],
   "source": [
    "# Cross validation on train data\n",
    "clf = ComplementNB()\n",
    "scores = cross_val_score(clf, train_X, train_Y, cv=4)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training + Predicting + Writing predictions to CSV file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to a CSV file!\n"
     ]
    }
   ],
   "source": [
    "clf = ComplementNB()\n",
    "clf.fit(train_X, train_Y)\n",
    "pred = clf.predict(test_X)\n",
    "d_pred = decode(pred)\n",
    "test_ids = get_test_data(test_filename)[1]\n",
    "write_csv(test_ids, d_pred, \"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
